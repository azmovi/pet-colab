# Lista de Exercícios 1

#### Professor
![Helio](https://img.shields.io/badge/Helio_Crestana_Guardia-%2300599C.svg?style=for-the-badge&logo=GoogleScholar&logoColor=white)


## Geral + SHM

### 1. Discuta o uso do paralelismo como solução para aumento de desempenho.

### 2. Considerando o particionamento de programas, descreva as técnicas de decomposição de domínio e decomposição funcional.

### 3. Descreva características do código de uma aplicação que podem ser usadas para a sua paralelização automática.

### 4. Relacione aspectos de aplicações paralelas que podem ser relevantes nos seus mapeamentos em uma determinada arquitetura paralela.

### 5. Considerando a arquitetura de um sistema computacional multiprocessado, discuta aspectos da interligação de processadores e memória que podem ser relevantes no desempenho de aplicações paralelas.

### 6. Considerando o posicionamento de dados na memória em um sistema multiprocessado, discuta aspectos que podem ser relevantes no desempenho de programas parelalos. 

### 7. Relacione mecanismos de comunicação e sincronização entre processos (IPC) em ambientes de programação com memória compartilhada.

### 8. Descreva mecanismos de comunicação e sincronização em ambientes com memória distribuída.

### 9. Comente a sobreposição computação / comunicação com a realização de operações de entrada e saída de dados assíncronas.

## Programação paralela com *OpenMP*

### 10. Como é definida uma região paralela com *OpenMP*?

### 11. Como é determinado o grau de paralelismo (número de tarefas) que será empregado num trecho de código paralelo?

### 12. O que fazem as chamadas *omp_get_num_*threads*()* e *omp_get_thread_num()*?

### 13. Para que serve e como funciona a diretiva single ? Como ela é aplicada numa região paralela?

### 14. Como funciona o modelo de divisão de trabalho com a diretiva sections no *OpenMP*?

### 15. Como funciona o modelo de divisão de trabalho com a diretiva for no *OpenMP*?

### 16. Como podem ser divididas as iterações de um *loop* for entre tarefas usando *OpenMP*?

### 17. Quais políticas podem ser usadas para essa atribuição e como funcionam?

### 18. Para que serve a cláusula de redução, por exemplo, quando aplicada a um for paralelo?

### 19. O que são *tasks* em *OpenMP*? Como são criadas e como se relacionam com as *threads* de uma região paralela?

### 20. Como são definidas variáveis privadas para cada thread de uma região paralela?

### 21. Como são definidas variáveis compartilhadas entre as *threads* de uma região paralela?

### 22. Como é possível prover exclusão mútua na manipulação de variáveis compartilhas entre as *threads* de uma região paralela?

### 23. Para que serve e como funciona a primitiva atomic em *OpenMP*?


## Contato

Se você tiver alguma dúvida, sugestão ou precisar de suporte, por favor, sinta-se à vontade para entrar em contato conosco:

- **E-mail:** petbcc.ufscar@gmail.com

Você também pode criar uma **Issue** no [GitHub](https://github.com/petbccufscar/pet-colab/issues) para relatar problemas, sugerir melhorias ou contribuir para o desenvolvimento do PET-COLAB. Estamos sempre abertos para receber feedback e colaboração. Obrigado!