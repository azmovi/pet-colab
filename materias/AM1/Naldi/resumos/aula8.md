# Resumo Aula 8
> Resumo criado pelo aluno [Jo√£o Pedro Trevisan](https://www.linkedin.com/in/joao-pedro-trevisan)

#### Professor
![Murilo Naldi](https://img.shields.io/badge/Murilo_Coelho_Naldi-%2300599C.svg?style=for-the-badge&logo=GoogleScholar&logoColor=white)

## Agrupamento por Densidade

At√© ent√£o foram estudados grupos formados por similaridade. Nesse tipo de agrupamento os objetos do mesmo grupo s√£o os mais similares e pode se dizer que √© um conceito bastante intuitivo visualmente (algumas vezes pode enganar, veja a aula passada).

M√©todos que focam em agrupamento por similaridade acabam encontrando grupos globulares com facilidade, por√©m agrupamentos que fogem desse formato s√£o bem mais dif√≠cil de serem identificados:

![AM1_aula09_img01.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img01.png)

Cria-se agora o conceito de **agrupamento por densidade** no qual observamos uma distribui√ß√£o de observa√ß√µes e definimos um **limiar de densidade** para "cortar" nosso gr√°fico e observar os grupos neste plano projetados. Grupos de alta densidade s√£o separados por regi√µes de baixa densidade.

![AM1_aula09_img02.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img02.png)

Tomemos um ponto cercado por um c√≠rculo de raio R:

![AM1_aula09_img03.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img03.png)

√â intuitivo ver qual das regi√µes possui maior densidade.

Ao passo que um algoritmo como o **K-means** tem um √∫nico par√¢metro *K* que definir√° o n√∫mero de grupos, um algoritmo de agrupamento por densidade tem como par√¢metros o raio da √°rea a ser observada e o n√∫mero de pontos que devem estar dentro deste raio para que haja um grupo. Disso se observa que ao informar raio e quantidade de pontos se est√° informando a densidade minima de um grupo a ser identificado.

**DBSCAN** √© o **Density-Based Spatial Clustering of Applications with Noise**. Ele √© o algoritmo de densidade mais popular do mundo com coisa de 25.500 cita√ß√µes, o Professor Naldi trabalhou por dois anos com o criador desse algoritmo.

No **DBSCAN** a densidade √© dada por dois par√¢metros: 

Œµ - Vizinhan√ßa: conjunto de pontos com dist√¢ncia m√°xima Œµ de um ponto de refer√™ncia *p*.

Mpts: n√∫mero m√≠nimo de pontos na Œµ-Vizinhan√ßa para considerar *p* como um ponto de regi√£o densa.

**NŒµ(p) = {q|d (p,q) <= Œµ}**

Nos slides 14 ~ 16 h√° um exemplo do funcionamento desse algoritmo.

Existe o conceito de **alcan√ßabilidade** (reachability) que diz que todo ponto na Œµ-vizinhan√ßa de um ponto de n√∫cleo *p* √© dito **diretamente alcan√ß√°vel** por densidade por *p*. Caso haja uma corrente de pontos **diretamente ancan√ß√°veis** de *p* a *q* dizemos que *q* √© **indiretamente alcan√ß√°vel** por *p*.

Para cada objeto do universo observado que n√£o tenha sido "classificado" encontramos todos pontos alcan√ß√°veis por densidade por um dado ponto *o* e atribua um novo grupo a todos esses pontos, se n√£o atribua *o* ao conjunto de *outliers*. O resultado do algoritmo √© algo desse tipo:

![AM1_aula09_img04.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img04.png)

e ent√£o (com Œµ ideal)

![AM1_aula09_img05.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img05.png)

√â preciso se atentar que a tendencia √© que os Mpts vizinhos mais pr√≥ximos possuam dist√¢ncias pareceidas quando estiverem no mesmo grupo. *Outliers* possuem a dist√¢ncia para seus vizinhos maior do que o ponto *core*. Exemplo disso:

![AM1_aula09_img06.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img06.png)

![AM1_aula09_img07.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img07.png)

Aqui Œµ = 10 parece apropriado.

A complexidade do **DBSCAN** em espa√ßo √© O(n) e em tempo √© O(n¬≤) pois se calcula a dist√¢ncia de todos para todos, mas esse tempo pode ser reduzido para O(nlogn) com algumas t√©cnicas.

As limita√ß√µes do **DBSCAN** se encontram quando um conjunto de dados tem m√∫ltiplas densidades:

![AM1_aula09_img08.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img08.png)

![AM1_aula09_img09.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img09.png)

Pensando na interpreta√ß√£o dos dados, podemos dizer que o par√¢metro Mpts √© um fator de suaviza√ß√£o que tornar√° as "classifica√ß√µes" mais ou menos abrangentes (menos *clumps* de *outliers*) e o Œµ √© o limiar de densidade:

![AM1_aula09_img10.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img10.png)

√â importante pensar que uma solu√ß√£o de √∫nica densidade pode n√£o refletir a estrutura dos dados como um todo:

![AM1_aula09_img11.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img11.png)

Para solucionar esta quest√£o precisamos usar um algoritmo hier√°rquico baseado em densidade. Se pensarmos no **DBSCAN** num contexto de grafo podemos pensar nos v√©rtices desse grafo como as observa√ß√µes densas e os v√©rtices adjacentes s√£o as observa√ß√µes diretamente alcan√ß√°veis em Œµ.

Tomemos um grafo ponderado do seguinte formato:

![AM1_aula09_img12.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img12.png)

Os v√©rtices s√£o as observa√ß√µes e os pesos das arestas representam o raio m√≠nimo Œµpq para que p e q sejam densos e **diretamente alcan√ß√°veis** por Œµpq. Existe um n√≠vel de densidade Œª = 1/Œµpq abaixo da qual qualquer aresta (p, q) atende a esses crit√©rios.

Neste vi√©s, os grupos s√£o componentes conexos de subgrafos do grafo original. As arestas mant√©m apenas os v√©rtices menores do que o limiar Œµ definido pelo usu√°rio. Dos v√©rtices s√£o mantidos apenas os que possuem grau maior que zero (que n√£o sejam ru√≠do). Na imagem a seguir est√° delineado um grupo no grafo:

![AM1_aula09_img13.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img13.png)

Podemos dar um passo al√©m: se podemos ordenar as arestas desse grafo e remov√™-las em ordem decrescente de peso Œµ √© poss√≠vel tirar um par√¢metro da m√£o do usu√°rio. N√£o s√≥ isso, criamos um algoritmo hier√°rquico que possui todos os agrupamentos poss√≠veis.

O nome disso √© **H**(ierarchical)**DBSCAN\***. 

Ressaltando um ponto importante antes de prosseguir: esses algoritmos trabalham com a dist√¢ncia entre os pontos, ou seja, s√£o relacionais.

O **HDBSCAN\*** √© um ***algoritmo hier√°rquico de liga√ß√£o simples*** sobre o espa√ßo de dist√¢ncias de alcance de densidade e por isso ele √© conhecido como **LS Robusto** (resistente a *outliers*). Isso se d√° pelo fato de que uma dist√¢ncia entre dois pontos s√≥ ser√° relevante se ela tiver relev√¢ncia para outros pares de pontos.

O **HDBSCAN\*** pode ser obtido calculando a **MST** de um grafo G no espa√ßo de dist√¢ncias de alcance.

No slide 35 tem a progress√£o de um gr√°fico de observa√ß√µes at√© sua **MST** e depois para um **dendrograma** com cores simbolizando as dist√¢ncias Œµ.

As imagens a seguir s√£o **gr√°ficos de alcan√ßabilidade**, as regi√µes mais densas s√£o representadas pelos vales na curva e indicam os agrupamentos. Isso pode ser expandido para *n* dimens√µes.

![AM1_aula09_img14.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img14.png)

Existem tamb√©m os **gr√°ficos de silhueta** que mostra como variam as densidades dos grupos e dessa forma mostra onde eles se dividem em subgrupos de densidade diferente:

![AM1_aula09_img15.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img15.png)

Tendo a hierarquia dos dados o usu√°rio pode realizar uma parti√ß√£o nessa hierarquia. Pode-se realizar o corte horizontal tradicional em um dendrograma:

![AM1_aula09_img16.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img16.png)

Considere agora que estejamos buscando uma **parti√ß√£o multidensidade**. O corte pode ser feito de outra maneira nas diferentes densidades encontradas:

![AM1_aula09_img17.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img17.png)

Dada uma hierarquia e seja ùõøi a escolho do grupo Ci, extrair um conjunto {Ci, Cj, ..., Ck} de grupos que maximiza o crit√©rio de qualidade:  ùêΩ ( ùõø1 , ùõø 2 , ... , ùõøùëõ )

Tal que forme uma parti√ß√£o:

( ùõøùëñ =1 ‚ãÄ ùõø ùëó =1 ) ‚áí ùë™ ùëñ ‚à™ ùë™ ùëó =‚àÖ  
ùõø ùëñ =0 ‚áí ‚àÉ ùõø ùëó =1:ùë™ ùëñ ‚äÇ ùë™ ùëó ‚ãÅ ùë™ ùëó ‚äÇ ùë™ ùëñ

O crit√©rio de otimiza√ß√£o pode ser decomposto como uma soma de termos tais que cada termo S(Ci) representa a medida de qualidade do grupo Ci

Para realizar essa sele√ß√£o de forma correta tomamos o crit√©rio de qualidade J calculado da forma:

ùêΩ = ‚àë  ùõøùëñ ‚àô ùëÜ ( ùë™ ùëñ )
ùëñ=1  
ùëõ 

Buscamos maximizar J.

Dada uma hierarquia de grupos representada por uma √°rvore, um m√©todo para estimar a qualidade de cada grupo deve ser encontrado. Tamb√©m devemos ter um m√©todo para agregar as qualidades estimadas a serem maximizadas. Buscamos escolher grupos que formem uma parti√ß√£o al√©m de maximizar a qualidade agregada.

Tomemos o problema de programa√ß√£o din√¢mica (de baixo pra cima). Queremos encontrar o m√°ximo global:

![AM1_aula09_img18.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img18.png)

Tomemos os n√≥s C10 e C11, a soma de seus valores √© maior que o valor de seu pai C8, sob este vi√©s devemos manter os grupos C10 e C11. Ao subir nessa √°rvore vemos que quebrar C2 em C4 e C5 gera grupos de qualidade 6, a soma de suas qualidades sendo 12 que √© maior que a qualidade 7 do pai C2 e da soma das qualidades dos filhos, ou seja, devemos manter separado at√© C4 e C5. Do outro lado da √°rvore C3 tem qualidade maior que a soma das qualidades de seus filhos e portanto devemos particionar a raiz s√≥ at√© C3 neste lado da √°rvore.

A estabilidade de grupo para hierarquias baseadas em densidade pode ser definida como **S(Ci) = excesso de massa relativo de grupo Ci**. Esta m√©trica √© dada por:

![AM1_aula09_img19.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img19.png)

Note que o cluster C2 tem maior estabilidade que C4 e C5 e portanto ser√° selecionado para a "classifica√ß√£o" o grupo C2. 

## Crit√©rios de Valida√ß√£o de Agrupamento

De maneira quantitativa, o procedimento de valida√ß√£o √© feito por meio de um √≠ndice e seus tipos podem ser:
+ **Externo**
+ **Interno**
+ **Relativo**

Os **√≠ndices externos** comparam parti√ß√µes diferentes e medem sua similaridade. Se forem id√™nticas o valor retornado √© 1.

O **√≠ndice externo** √© utilizado para verificar a qualidade de um agrupamento de acordo com uma refer√™ncia, a informa√ß√£o √© **externa** aos dados.

Alguns desse √≠ndices s√£o:
+ Rand Index
+ Jaccard
+ Rand Index Ajustado
+ Fowlkes-Mallows
+ Estat√≠stica G
+ Nomralized Mutual Information

Estes √≠ndices se baseiam em algo muito semelhante com os componentes de uma matriz de espalhamento:

**MM**: N¬∫ de pares que pertencem ao mesmo grupo em ambas parti√ß√µes  
**MD**: N¬∫ de pares que pertencem ao mesmo grupo apenas na parti√ß√£o 1  
**DM**: N¬∫ de pares que pertencem ao mesmo grupo apenas na parti√ß√£o 2  
**DD**: N¬∫ de pares que n√£o pertencem ao mesmo grupo em ambas parti√ß√µes

O **Rand Index** √© dado por: 

**Rand(P1, P2) = (MM + DD)/(MD + DM + MM + DD)**

Note como ele √© parecido com o crit√©rio de **precis√£o** da classifica√ß√£o.

O **√çndice de Jaccard** √© dado por:

**Jc(P1, P2) = MM / (MD + DM + MM)**

Ele √© semelhante ao f measure da calssifica√ß√£o.

Existe tamb√©m o **Rand Ajustado** que soluciona o problema de que para **jaccard** e **rand** n√£o se espera um valor nulo para 2 parti√ß√µes completamente aleat√≥rias:

![AM1_aula09_img20.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img20.png)

Os **√≠ndices internos** s√£o usados quando n√£o se disp√µe de parti√ß√µes de refer√™ncia para validar a estrutura de grupos obtida, ou seja, existe apenas os dados e o resultado a ser avaliado. 

Como exemplo podemos tomar a fun√ß√£o objetivo do K-means conhecida como **SSE**: **Sum of squared errors** que mede a vari√¢ncia intra cluster. Ela √© dada por:

![AM1_aula09_img26.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img26.png)

Nesta equa√ß√£o *d* √© a dist√¢ncia euclidiana. O centr√≥ide de do c-√©simo grupo √© encontrado pela equa√ß√£o que segue:

![AM1_aula09_img27.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img27.png)


J√° os **√≠ndices relativos** fazem refer√™ncia a uma classe de √≠ndices capaz de indicar qual a melhor dentre duas ou mais parti√ß√µes, a caracteriza√ß√£o como **relativo** pode depender n√£o s√≥ do crit√©rio mas tamb√©m as vezes do contexto. Como exemplo disso o SSE √© um crit√©rio relativo se as parti√ß√µes a serem comparadas possuem o mesmo n√∫mero de grupos.

**√çndices relativos**, num contexto amplo, s√£o definidos como √≠ndices capazes de avaliar individualmente uma parti√ß√£o e quantificar esta avalia√ß√£o atrav√©s de um valor que possa ser comparado relativamente.

Consideremos o exemplo de um **√≠ndice relativo I(  
P)** que √© proporcional a qualidade do agrupamento.

![AM1_aula09_img28.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img28.png)

**√çndices relativos** s√£o mais flex√≠veis pois podem ser usados como crit√©rios de otimiza√ß√£o para se escolher melhores parti√ß√µes e podem servir como regras de parada para se n√£o se passar de um determinado valor de qualidade.

Um bom exemplo de **√≠ndice relativo** e **interno** √© o **√≠ndice Dunn** que toma a maior dist√¢ncia inter-grupo e a maior dist√¢ncia intra-grupo:

![AM1_aula09_img21.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img21.png)

No √≠ndice original, as dist√¢ncias inter-grupos e a intra-grupos s√£o calculadas segundo liga√ß√£o simples e di√¢metro m√°ximo o que o torna muito sens√≠vel a *outliers*.

**√≠ndice silhueta**: Esse √≠ndice se baseia nas caracter√≠sticas relativas a forma com a qual um objeto foi agrupado, quanto maior a dist√¢ncia do objeto para os outros grupos (b(Xi)) e menor a dist√¢ncia para seu grupo (a(Xi)), melhor ser√° a avalia√ß√£o. A **silhueta** de um objeto √© dada por:

![AM1_aula09_img22.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img22.png)

A **silhueta de um grupo/parti√ß√£o** √© dada pela m√©dia das **silhuetas** de seus objetos:

![AM1_aula09_img23.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img23.png)

Existem diferentes tipos de **silhueta** a serem usados dependendo da situa√ß√£o segundo os valores de (a(Xi)) e (b(Xi)):
+ **a(Xi)**: dissimilaridade m√©dia do i-√©simo objeto em rela√ß√£o aos objetos de seu grupo.
+ **b(Xi)**: dissimilaridade m√©dia do i-√©simo objeto em rela√ß√£o aos objetos do grupo mais pr√≥ximo a que **Xi** ***n√£o*** ***pertence**.

Pode-se ainda simplificar a **silhueta** considerando apenas a dist√¢ncia do objeto ao centr√≥ide do seu grupo (a(Xi)) ou ao centr√≥ide do grupo mais pr√≥ximo que n√£o seja o seu (b(Xi)). Isso n√£o √© uma solu√ß√£o relacional, mas sua complexidade √© linear e n√£o quadr√°tica.

Existem muitos outros √≠ndices pelos quais se pode avaliar as parti√ß√µes feitas, a imagem a seguir lista alguns deles:

![AM1_aula09_img24.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img24.png)

Vale lembrar que para **algoritmos hier√°rquicos** o processo de valida√ß√£o dos agrupamentos podemos escolher uma de suas parti√ß√µes e aplicar o crit√©rio relativo em qualquer um dos n√≠veis da hierarquia e comparar os resultados em diferentes n√≠veis.

Isso √© muito √∫til para se avaliar a melhor quantidade de parti√ß√µes que um par algoritmo-dataset tem. A tabela a baixo mostra diferentes algoritmos hier√°rquicos e diferentes quantidades de parti√ß√µes, veja como √© f√°cil avaliar:

![AM1_aula09_img25.png](https://raw.githubusercontent.com/petbccufscar/.github/main/pet-colab/AM1/AM1_aula09_img25.png)

Em suma, √≠ndices de valida√ß√£o avaliam a qualidade de um agrupamento em compara√ß√£o com uma estrutura conhecida ou quantitativamente ou ainda em rela√ß√£o a outro agrupamento. Eles s√£o frequentemente usados para selecionar agrupamentos e diferentes √≠ndices podem resultar em avalia√ß√µes bastante distintas.


## Contato

Se voc√™ tiver alguma d√∫vida, sugest√£o ou precisar de suporte, por favor, sinta-se √† vontade para entrar em contato conosco:

- **E-mail:** petbcc.ufscar@gmail.com

Voc√™ tamb√©m pode criar uma **Issue** no [GitHub](https://github.com/petbccufscar/pet-colab/issues) para relatar problemas, sugerir melhorias ou contribuir para o desenvolvimento do PET-COLAB. Estamos sempre abertos para receber feedback e colabora√ß√£o. Obrigado!
